TABLE SCHEMAS MODELLED FROM dbdiagram.io:

Table users {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  username VARCHAR(30)
  bio VARCHAR(400)
  avatar VARCHAR(200)
  phone VARCHAR(25)
  email VARCHAR(40)
  password VARCHAR(50)
  status VARCHAR(15)
}

Table comments {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  content VARCHAR(240)
  user_id INTEGER [ref: > users.id]
  post_id INTEGER [ref: > posts.id]
}

Table posts {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  url VARCHAR(200)
  user_id INTEGER [ref: > users.id]
  caption VARCHAR(240)
  lat REAL
  lng REAL
}

Table likes {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  user_id INTEGER [ref: > users.id]
  post_id INTEGER [ref: > posts.id]
  comment_id INTEGER [ref: > comments.id]
}

Table photo_tags {
  id INTEGER [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  post_id INTEGER [ref: > posts.id]
  user_id INTEGER [ref: > users.id]
  x INTEGER
  Y integer
}

Table caption_tags {
  id INTEGER [pk, increment]
  created_at TIMESTAMP
  updated_at TIMESTAMP
  post_id INTEGER [ref: > posts.id]
  user_id INTEGER [ref: > users.id]
}

Table hashtags {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  title VARCHAR(20)
}

Table hashtags_posts {
  id SERIAL [pk, increment]
  post_id INTEGER [ref: > posts.id]
  hashtag_id INTEGER [ref: > hashtags.id]
}

Table followers {
  id SERIAL [pk, increment]
  created_at TIMESTAMP
  leader_id INTEGER [ref: > users.id]
  follower_id INTEGER [ref: > users.id]
}








CREATE A NEW DATABASE CALLED instagram IN PGADMIN BY RIGHT-CLICKING DATABASES -> CREATE -> DATABASE -> ENTER NAME -> SAVE (WITH DEFAULT OPTIONS)
RIGHT CLICK THE NEWLY CREATED DATABASE AND THEN CLICK Query Tool TO EXECUTE SQL COMMANDS. THIS QUERY TOOL IS SPECIFIC TO THE DATABASE THAT YOU OPENED IT ON    .



CREATING TABLES:



-- CREATE TABLE users (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	username VARCHAR(30) NOT NULL,
-- 	bio VARCHAR(400),
-- 	avatar VARCHAR(200),
-- 	phone VARCHAR(25),
-- 	email VARCHAR(40),
-- 	password VARCHAR(50),
-- 	status VARCHAR(15),
-- 	CHECK(COALESCE(phone, email) IS NOT NULL)
-- );



-- CREATE TABLE posts (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	url VARCHAR(200) NOT NULL,
-- 	caption VARCHAR(240),
-- 	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	lat REAL CHECK(lat IS NULL OR (lat >= -90 AND lat <= 90)),
-- 	lng REAL CHECK(lng IS NULL OR (lng >= -180 AND lng <= 180))
-- );



-- CREATE TABLE comments (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	contents VARCHAR(240) NOT NULL,
-- 	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE
-- );



-- CREATE TABLE likes (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	post_id INTEGER REFERENCES posts(id) ON DELETE CASCADE,
-- 	comment_id INTEGER REFERENCES comments(id) ON DELETE CASCADE,
-- 	CHECK(
-- 		COALESCE((post_id)::BOOLEAN::INTEGER, 0) 
-- 		+	 
-- 		COALESCE((comment_id)::BOOLEAN::INTEGER, 0) 
-- 		= 1
-- 	),
-- 	UNIQUE(user_id, post_id, comment_id)
-- );



-- CREATE TABLE photo_tags (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
-- 	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	x INTEGER NOT NULL,
-- 	y INTEGER NOT NULL,
-- 	UNIQUE(user_id, post_id)
-- );



-- CREATE TABLE caption_tags (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
-- 	UNIQUE(user_id, post_id)
-- );



-- CREATE TABLE hashtags (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	title VARCHAR(20) NOT NULL,
-- 	UNIQUE(title)
-- );



-- CREATE TABLE hashtags_posts (
-- 	id SERIAL PRIMARY KEY,
-- 	hashtag_id INTEGER NOT NULL REFERENCES hashtags(id) ON DELETE CASCADE,
-- 	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
-- 	UNIQUE(hashtag_id, post_id)
-- );
	
	
	
-- CREATE TABLE followers (
-- 	id SERIAL PRIMARY KEY,
-- 	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
-- 	leader_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	follower_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
-- 	CHECK(leader_id != follower_id),
-- 	UNIQUE(leader_id, follower_id)
-- );



LOAD SEED DATA FROM postgres_ig_seed.sql FILE:
RIGHT CLICK THE instagram DATABASE THAT WE CREATED -> RESTORE -> BROWSE AND SELECT THE postgres_ig_seed.sql FILE.
GO TO "Data Options" SECTION:
	NEXT, IN THE "Type of Objects" SECTION, SELECT "Only Data" TO YES.
	NEXT, IN THE "Do not Save" SECTION, SELECT "Owner" TO YES.
GO TO "Query Options" SECTION:
	NEXT, SELECT "Single Transcation" TO YES.
GO TO "Options" SECTION:
	NEXT, IN THE "Disable" SECTION, SELECT "Triggers" TO YES.
	NEXT, IN THE "Miscellaneous/Behaviour" SECTION, SELECT "Verbose Messages" TO YES.
FINALLY CLICK ON THE Restore BUTTON TO RESTORE THE DATA (WILL TAKE A COUPLE OF SECONDS/MINUTES).



IF YOU NEED TO RESTORE THE DATABASE TO THIS INITIAL STATE IN FUTURE, CLOSE ANY OPEN QUERY TOOL WINDOWS AND THEN GO TO THE DASHBOARD.
FROM THE SERVER ACTIVITY -> SESSIONS TAB, TERMINATE ALL OTHER SESSIONS EXCEPT FOR THE DATABASE CONNECTION SESSION.
THEN RIGHT-CLICK ON THE DATABASE FROM THE LEFT PANE AND DELETE/DROP THE DATABASE. THEN RECREATE A FRESH NEW DATABASE WITH THE SAME NAME.
NOW RESTORE DATABASE FROM THE postgres_ig_seed.sql FILE AS BEFORE,
EXCEPT THIS TIME, IN THE "Data Options" SECTION, "Only Data" AND "Only Schema" BOTH SHOULD NOT BE SELECTED AS WE ARE GOING TO IMPORT BOTH THE TABLES AND THEIR DATA.




QUERIES:


-- SELECT COUNT(*) FROM users;

-- SELECT COUNT(*) FROM likes;

-- SELECT * FROM users
-- ORDER BY id DESC
-- LIMIT 3;

-- SELECT username, caption
-- FROM users JOIN posts 
-- ON users.id = posts.user_id
-- WHERE users.id = 200;

-- SELECT username, COUNT(*) AS likes
-- FROM likes JOIN users
-- ON likes.user_id = users.id
-- GROUP BY username;




POSTGRES INTERNALS:
TO VIEW CONFIGURATION OPTIONS IN POSTGRES, WE USE THE "SHOW" COMMAND
WE CAN VIEW THE LOCATION WHERE POSTGRES IS INSTALLED USING
-- SHOW data_directory;
FROM HERE, GO TO data/base TO VIEW ALL THE DATABASE RELATED DATA

USE THIS COMMAND TO FIND OUT THE DATABASE TO NUMERIC ID MAPPING
-- SELECT oid, datname
-- FROM pg_database;


FIND WHAT EACH BINARY DATA FILE INSIDE THE DATABASE DIRECTORY CONTAINS:State changes should be logged before any heavy update to permanent storage
-- SELECT * FROM pg_class;

NOTE THAT POSTGRES STORES ALL THE DATA IN FILES IN THE FORM OF BINARY IN THE STORAGE DISK.
FIND THE ROW WITH relname SET AS users FROM THE QUERY RESULT TO GET THE FILE THE CONTAINS THE users TABLE DATA.
SPECIFICALLY, THIS FILE IS CALLED A HEAP / HEAP FILE THAT STORES ALL THE ROWS OF A PARTICULAR TABLE.
THE HEAP FILE IS DIVIDED INTO MANY DIFFERENT BLOCKS/PAGES. EACH BLOCK/PAGE STORES SOME NUMBER OF ROWS FROM THE HEAP.
A BLOCK FILE IS ALWAYS 8KB (8096 BYTEs) IN SIZE REGARDLESS OF HOW MANY ROWS/TUPLES/ITEMS OF DATA IT CONTAINS (EVEN WHEN EMPTY, 
IT IS 8KB IN SIZE). 
EACH BLOCK/PAGE CONTAINS MANY DIFFERENT TUPLE/ITEMS/ROWS. A TUPLE/ITEM CONTAINS AN INDIVIDUAL ROW FROM A TABLE. 
YOU CAN VISUALIZE A HEAP FILE IN A HEX-EDITOR. THIS WILL SHOW THE HEX-CODE OF EACH BYTE STORED IN THE HEAP FILE, THE ACTUAL DECODED
TEXT AS WELL AS BINARY FORM OF THE BYTE. 

KEEP IN MIND THAT YOU WILL REACH THE SECOND BLOCK/PAGE AFTER THE FIRST 8096 HEX CODES. THE HEX EDITOR USUALLY SHOWS 16 BYTES IN A ROW.
The first 24 bytes of each page consists of a page header (PageHeaderData). Out of this 24, the first 8 bytes contain the WAL record for changes to the page.
PostGres uses WAL method to log State changes before any heavy update to the actual data in the permanent storage file.
The next 2 bytes are reserved for the checksum (hash cipher) of the page block.
Next is a 2-byte field containing flag bits
Next 6 bytes contain byte offsets from the page start to the start of unallocated space, to the end of unallocated space, and to the start of the special space
start and unallocated space end. These are stored as three 2 byte integer fields (pd_lower, pd_upper, and pd_special).
The next 2 bytes store page size and layout version of the page/block.
The last 2 bytes stores the last field as a hint that shows whether pruning the page is likely to be profitable: it tracks the oldest un-pruned XMAX on the page
All these details can be found in "src/include/storage/bufpage.h".


Following the page header are item identifiers (ItemIdData), each requiring four bytes. An item identifier contains a byte-offset to the start of an item, its length in bytes, and a few attribute bits which affect its interpretation. New item identifiers are allocated as needed from the beginning of the unallocated space. The number of item identifiers present can be determined by looking at pd_lower, which is increased to allocate a new identifier. Because an item identifier is never moved until it is freed, its index can be used on a long-term basis to reference an item, even when the item itself is moved around on the page to compact free space. In fact, every pointer to an item (ItemPointer, also known as CTID) created by PostgreSQL consists of a page number and the index of an item identifier.

The items themselves are stored in space allocated backwards from the end of unallocated space. New item identifiers are allocated from the start of the unallocated space, new items from the end. The exact structure varies depending on what the table is to contain. Tables and sequences both use a structure named HeapTupleHeaderData, described below.

The final section is the “special section” which can contain anything the access method wishes to store. For example, b-tree indexes store links to the page's left and right siblings, as well as some other data relevant to the index structure. Ordinary tables do not use a special section at all (indicated by setting pd_special to equal the page size).
REFER: https://www.postgresql.org/docs/current/storage-page-layout.html


		|------------------------------|------------------------|-------------------|-------------------------------------------|
		|			       |			|	itemID	    |    					|
		|	PageHeaderData	       |	ItemID		|		    |---------------------->>			|
		|------------------------------|-----------|------------|----------|--------|						|
		|					   | 			   |							|
		|					   |-----------------------|--------| 						|
		|					   			   |	    |						|
		|						         |---------|	    |						|
		|							 |		    |						|
		|							 |	 	    |	 					|
		|							 |		    |						|
		|							 V		    V						|
		|						   |----------------|------------------|--------------------------------|
		|					<<---------|		    |	  Item	       |				|
		|						   |	Item	    |		       |	Special			|
		|--------------------------------------------------|----------------|------------------|--------------------------------|


All table rows are structured in the same way. There is a fixed-size header (occupying 23 bytes on most machines), followed by an optional null bitmap, an optional object ID field, and the user data. The actual user data (columns of the row) begins at the offset indicated by t_hoff, which must always be a multiple of the MAXALIGN distance for the platform. The null bitmap is only present if the HEAP_HASNULL bit is set in t_infomask. If it is present it begins just after the fixed header and occupies enough bytes to have one bit per data column (that is, the number of bits that equals the attribute count in t_infomask2). In this list of bits, a 1 bit indicates not-null, a 0 bit is a null. When the bitmap is not present, all columns are assumed not-null. The object ID is only present if the HEAP_HASOID_OLD bit is set in t_infomask. If present, it appears just before the t_hoff boundary. Any padding needed to make t_hoff a MAXALIGN multiple will appear between the null bitmap and the object ID. (This in turn ensures that the object ID is suitably aligned.)
All the details can be found in "src/include/access/htup_details.h".

Interpreting the actual data can only be done with information obtained from other tables, mostly pg_attribute. The key values needed to identify field locations are attlen and attalign. There is no way to directly get a particular attribute, except when there are only fixed width fields and no null values. All this trickery is wrapped up in the functions heap_getattr, fastgetattr and heap_getsysattr.

To read the data you need to examine each attribute in turn. First check whether the field is NULL according to the null bitmap. If it is, go to the next. Then make sure you have the right alignment. If the field is a fixed width field, then all the bytes are simply placed. If it's a variable length field (attlen = -1) then it's a bit more complicated. All variable-length data types share the common header structure struct varlena, which includes the total length of the stored value and some flag bits. Depending on the flags, the data can be either inline or in a TOAST table; it might be compressed, too.

THE CONCATENATED BINARY VALUES UNDER THE '0C' and '0D' COLUMNS IN FIRST ROW OF THE HEX EDITOR, CORRESPONDS TO AN uint16 INTEGER THAT SPECIFIES THE NUMBER OF BYTES FROM THE PAGE START TO 
SKIP TO REACH THE FREE-SPACE PART OF THE PAGE/BLOCK. SIMILARLY, THE '0E' and '0F' COLUMNS OF THE FIRST ROW IN HEX EDITOR SPECIFY THE NUMBER OF BYTES TO SKIP FROM THE START OF THE PAGE/BLOCK TO REACH THE END OF THE FREE-SPACE AND THE BEGINING OF THE ITEM/TUPLE (ROW) DATA ITSELF. 
SIMILARLY, YOU CAN TRACE OUT THE ITEM-IDs SECTION OF THE PAGE AND GET THE BYTE-OFFSET TO REACH THE START OF THE ITEM/TUPLE/ROW DATA AS WELL AS THE LENGTH OF THE ITEM/TUPLE DATA
IN BYTES. KEEP IN MIND THAT THE ORDER OF ITEM-ID TO ITEM MAPPING IS REVERSED: THE LAST ITEM-ID CORRESPONDS TO THE FIRST ITEM/TUPLE STORED IN THE PAGE/BLOCK. 

IF WE GET TO THE FIRST ROW/TUPLE DATA AND SKIP THE FIRST HEADER WITH 23 + OPTIONAL BYTES, WE WILL GET THE ID OF THE ACTUAL ROW:

SELECT * FROM users WHERE username = 'Gene76'

YOU WILL SEE THAT THE ID RETURNED MATCHES THE uint16 DATA FOR THE STARTING BYTE OF THE ITEM/TUPLE!


POSTGRES ACTUALLY LOADS THESE HEAP FILES INTO MEMORY (RAM) AND ITERATES OVER THE RECORDS ONE-BY-ONE IN ORDER TO EXECUTE QUERIES. THIS IS CALLED AS A "FULL TABLE SCAN".
THIS HAS A HUGE IMPACT ON THE PERFORMANCE OF QUERIES. THIS IS WHERE "INDEXES" COME INTO PICTURE. AN INDEX IS A TREE-LIKE DATA STRUCTURE THAT EFFICIENTLY TELLS WHAT POSITION IN A 
BLOCK/PAGE, IS THE RECORD STORED AT! USING AN INDEX, WE CAN LOAD UP ONLY THE BLOCKS REQUIRED TO EXECUTE THE QUERY (LOAD ONLY THE BLOCKS WITH AFFECTED RECORDS INTO RAM)!
INDEXES ARE CREATED FOR SPECIFIC COLUMNS OF THE TABLE ON WHICH YOU WANT TO PERFORM FAST LOOKUPS! ALL THE INDEX DATA IS SORTED BY SOME ORDER AND THEN EVENLY DISTRIBUTED
INTO A TREE DATA STRUCTURE IN LEFT-TO-RIGHT ORDER. EVERY NODE IN THIS TREE DATA STRUCTURE WILL HAVE A CONDITIONAL THAT HELPS TO TRAVERSE THE TREE AND FIND
THE HEAP FILE TO LOAD AS WELL AS A POINTER TO THE START OF THE ITEM/TUPLE/ROW DATA IN THE CORRESPONDING BLOCK/PAGE WITHIN THE HEAP FILE.
POSTGRES AUTOMATICALLY CREATES AN INDEX FOR THE PRIMARY KEY COLUMN OF EVERY TABLE! ALSO FOR COLUMNS WITH THE "UNIQUE" CONSTRAINT! BUT THESE INTERNAL INDEXES
WILL NOT BE LISTED IN PGADMIN!

-- SEE INTERNAL INDEXES CREATED BY POSTGRES FOR DATABASE:
SELECT relname, relkind FROM pg_class
WHERE relkind = 'i';

-- CREATING AN INDEX (DEFAULT NAMING CONVENTION - users_username_idx):
CREATE INDEX ON users (username);

-- CREATING AN INDEX (MANUAL NAMING CONVENTION - my_idx):
CREATE INDEX my_idx ON users (bio);

-- DELETE AND INDEX:
DROP INDEX my_idx;

-- BENCHMARKING QUERIES:
-- WITH INDEX: 0.07ms
EXPLAIN ANALYZE SELECT * FROM users
WHERE username = 'Emil30';

-- DELETE THE INDEX
DROP INDEX users_username_idx;

-- QUERY PERFORMANCE WITHOUT INDEX: 0.7ms - UPTO 10X MORE TIME!
EXPLAIN ANALYZE SELECT * FROM users
WHERE username = 'Emil30';


NOTE THAT INDEXES THEMSELVES ARE ACTUALLY STORED AS FILES IN PERMANENT STORAGE.
-- STORAGE SIZE OF A TABLE:
SELECT pg_size_pretty(pg_relation_size('users'));

-- STORAGE SIZE OF AN INDEX:
CREATE INDEX ON users (username);
SELECT pg_size_pretty(pg_relation_size('users_username_idx'));

DOWNSIDES OF INDEXES:
* INDEXES CAN BE RATHER LARGE IN SIZE AS THEY STORE ALL THE DATA OF ATLEAST ONE COLUMN OF THE TABLE
* INDEXES SLOW DOWN INSERT/DELETE/UPDATE OPERATIONS AS THEY HAVE TO BE UPDATED ALONG WITH THE TABLE IN SUCH CASES
* SOME INDEXES MAY BE CREATED, BUT NEVER USED, LEADING TO WASTAGE IN STORAGE SPACE


THE B-TREE DATA STRUCTURE IS USED FOR CREATING INDEXES IN THE VAST MAJORITY OF THE TIME. HOWEVER, THERE ARE CERTAIN SPECIAL CASES WHERE OTHER TYPES OF DATA STRUCTURES
ARE USED TO CREATE INDEXES:
* HASH: SPEED UP SIMPLE EQUALITY CHECKS
* GIST: OPTIMIZED FOR GEOMETRY, FULL TEXT SEARCH
* SP-GIST: OPTIMIZED FOR CLUSTERED DATA SUCH AS DATES WITH THE SAME YEAR ETC.
* GIN: FOR COLUMNS WITH ARRAYS OR FOR JSON DATA
* BRIN: SPECIALIZED FOR REALLY LARGE DATABASES

THE INDEX FILE STORED IN PERMANENT STORAGE ALSO HAS A SIMILAR ARCHITECTURE TO A HEAP FILE. IT CONSISTS OF 8KB SIZE BLOCKS WITH SAME STRUCTURE AS HEAP FILE BLOCKS. 
HOWEVER IN AN INDEX, EACH BLOCK/PAGE HAS A VERY SPECIFIC PURPOSE. THE BLOCK/PAGES ARE:
* A META PAGE WHICH STORES INFORMATION ABOUT THE INDEX (8KB)
* A ROOT PAGE (8KB) THAT DIRECTS TO LEAF PAGES
* MANY NUMBER OF LEAF PAGES (8KB) THAT STORES THE ACTUAL INDEX INFORMATION

-- CREATE NEW FUNCTIONALITY TO EXPLORE DATA ON EACH PAGE OF AN INDEX:
CREATE EXTENSION pageInspect;
-- EXPLORE THE B-TREE FOR METAPAGE IN INDEX FILE
SELECT * FROM bt_metap('users_username_idx');

-- USE ROOT PAGE INDEX FROM PREVIOUS META PAGE QUERY TO GET LEAF PAGE INFORMATION (THE ctid COLUMN IN ROOT PAGE CORRESPONDS TO POSITION OF THE LEAF PAGES OF THE INDEX FILE)
-- THE data COLUMN CONTAINS THE HEXCODE OF ACTUAL ITEMS OF THE COLUMN THAT THE INDEX IS CREATED FOR (users.username IN THIS CASE). 
-- NOTICE THAT THE FIRST ROW OF data IS EMPTY. THIS IS BECAUSE, THE INDEX WILL BE CREATED USING A SORTED ORDER OF THE COLUMN ITEMS AND IF A CONDITION IS
-- NOT SATISFIED FOR A ROW, THE cid INDEX OF LEAF PAGE FROM THE PREVIOUS ROW WILL BE USED TO LOAD UP THE CORRESPONDING BLOCK DATA FROM THE INDEX FILE.
-- EACH data ROW IS CHECKED WITH A CONDITIONAL. WHEN THIS CONDITIONAL IS SATISFIED FOR THE QUERY WITH ANY OF THE ROWS IN THE INDEX, THE CORRESPONDING
-- ctid IS USED TO LOAD THE CORRESPONDING BLOCK/PAGE FOR THE QUERY. THE FIRST ROW IN THE ROOT PAGE HAS NO DATA CONDITIONAL BECAUSE IT HAS TO ALLOW
-- ALL INCOMING DATA TO BE CHECKED WITH THE CONDITIONAL IN THE SECOND ROW AND ONLY THE QUERY FAILING THE SECOND ROW CONDITIONAL WILL UTILIZE THE LEAF BLOCK/PAGE INDEX
-- FROM THE FIRST ROW OF THE ROOT PAGE TO GET THE INDEX OF THE LEAF PAGE. THE ctid COLUMN IN THE LEAF PAGE/BLOCK POINTS TO THE CORRESPONDING BLOCK/PAGE INDEX AND A
-- POINTER TO THE REQUIRED DATA IN THE ACTUAL HEAP FILE OF THE TABLE! HOWEVER, THE FIRST ROW IN EACH LEAF PAGE IS A POINTER TO THE FIRST DATA ROW (SECOND ROW) OF THE SUCCESSIVE
-- LEAF PAGE (KEEP IN MIND THAT THE LEAF PAGES ARE FOLLOW A LEFT->RIGHT ORDER IN THE B-TREE) AND IT WILL BE IGNORED WHEN SEARCHING FOR THE ctid OF AN ITEM IN THE
-- CURRENT LEAF NODE! SINCE THERE CAN ONLY BE ONE ROOT PAGE WITH ctid OF CORRESPONDING LEAF PAGES, THE FIRST ROOT PAGE ROW HAS NO POINTERS TO OTHER PAGES AND IS LEFT EMPTY.


-- CHECK ROOT PAGE FOR ctid COINTAINING POSITION AND POINTER OF LEAF PAGE WITHIN INDEX FILE FOR DATA QUERY PASSING CORRESPONDING CONDITIONAL!
SELECT * FROM bt_page_items('users_username_idx', 3);

-- CHECK LEAF PAGE ctid FOR ACTUAL BLOCK/PAGE INDEX AND POINTER TO THE ITEM/TUPLE WITHIN THAT BLOCK IN THE HEAP FILE!
SELECT * FROM bt_page_items('users_username_idx', 1);

-- VERIFYING THE ctid FROM LEAF PAGE WITH IMPLICIT ctid FROM THE TABLE:
-- GET THE BLOCK/PAGE AND INDEX FOR LOADING THE CORRESPONDING HEAP FILE ASSOCIATED WITH users.username = 'Aaliyah.Hintz' FROM THE users TABLE USING IN-BUILT ctid
-- THE ctid COLUMN IS CREATED IMPLICITLY IN THE users TABLE WITHIN POSTGRES!
SELECT ctid, * FROM users WHERE username = 'Aaliyah.Hintz';


						|---------------------------------------------------------------------------|
						|	                               PAGE 3                               |
						|                                    ROOT NODE                              |
						|----------------------------|--------------------------|-------------------|
						|                            |    ALL >= 'Alyson14'     | ALL >= 'Austin.O' |
						|			     |	    ctid (2, x)	        |    ctid (4, x)    |
						|-------------|--------------|-------------|------------|---------|---------|
							      |				   |			  |
				|-----------------------------|				   |			  |
				|							   |			  |
				V							   |			  |
|---------------------------------------------------------------------------|		   |			  |
|	                               PAGE 1                               | 		   |			  |
|                                    LEAF NODE                              |		   |			  |
|----------------------------|--------------------------|-------------------|		   |			  |
|       'Aaliyah.H'          |                          |    'Alyson.R'     |		   |			  |
|	ctid (33, 43)	     |	         	        |    ctid (82, 30)  |		   |			  |
|----------------------------|--------------------------|-------------------|		   |			  |
|	(ROW 1)										   |			  |
|-------------|				 |-------------------------------------------------|			  |					
	      |		   		 |									  |
              |	 			 v									  |
|-------------|-------------------------------------------------------------|					  |
|	      |                        PAGE 2                               |					  |
|             |                      LEAF NODE                              |			   		  |
|-------------V--------------|--------------------------|-------------------|					  |
|       'Alyson14'           |                          |    'Austin.M'     |					  |
|	ctid (20, 7)	     |	         	        |    ctid (74, 18)  |				   	  |
|----------------------------|--------------------------|-------------------|					  |
|														  |
|	(ROW 1)													  |
|-------------|				|-------------------------------------------------------------------------|
	      |				|
	      |				v
|-------------|-------------------------------------------------------------|
|	      |                        PAGE 4                               |
|             |                      LEAF NODE                              |
|-------------V--------------|--------------------------|-------------------|
|       'Austin.O'           |                          |    'Beth.T'       |
|	ctid (84, 13)	     |	         	        |    ctid (99, 8)   |
|----------------------------|--------------------------|-------------------|


-- FOR TABLES WITH TONS OF RECORDS, THE ABOVE TREE ARCHITECTURE WILL ENCOMPASSS THREE OR MORE LEVELS OF NODES WITH THE FIRST LEVEL HAVING A ROOT NODE,
-- PARENT NODES IN THE INTERMEDIATE LEVELS AND THE LAST LEVEL HAVING THE LEAF NODES!


-- OBTAIN INDEX FILE NAME FROM PERMANENT STORAGE FOR THE instagram TABLE
SELECT oid, datname FROM pg_database;

-- GET LOCAL DIRECTORY FOR STORAGE IN POSTGRES INSTALLATION
SHOW data_directory;

-- GET ACTUAL FILENAME oid IN THE data_directory FOR INDEX OF users.username
SELECT * FROM pg_class 
WHERE relkind = 'i';


WHEN YOU FEED A QUERY INTO POSTGRES, IT GOES THROUGH A SERIES OF PROCESSING STEPS:
* PARSER: VALIDATES THE QUERY - CHECKS KEYWORDS AND BUILDS A QUERY TREE - A PROGRAMMATIC DESCRIPTION OF THE QUERY TO RUN
* REWRITER: APPLIES MODIFICATIONS TO THE QUERY FOR ENHANCED EFFICIENCY AND DECOMPOSES VIEWS INTO UNDERLYING TABLE REFERENCES
* PLANNER: USES DIFFERENT STRATEGIES TO FETCH THE QUERIED INFORMATION (LIKE USING INDEX) AND EVALUATES THE PLANS TO CHOOSE THE MOST EFFICIENT
* EXECUTOR: RUN THE QUERY

-- THE "EXPLAIN" AND "EXPLAIN ANALYZE" STATEMENTS IN POSTGRE-SQL HELP US IN BENCHMARKING QUERIES AND SHOW THE PLANNER'S OUTPUT
-- THE "EXPLAIN" STATEMENT BUILDS A PLAN FOR THE GIVEN QUERY AND DISPLAYS INFO ABOUT THE PLAN
-- THE "EXPLAIN ANALYZE" STATEMENT BUILDS A QUERY PLAN, RUNS IT, AND THEN PROVIDES INFO ABOUT IT!
-- THESE STATEMENTS DONOT RETURN THE ACTUAL QUERIED DATA, BUT RATHER PROVIDE BENCHMARKING INFO!

-- ACTUAL QUERY:
SELECT username, contents FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';

-- VIEW QUERY PLAN FROM PLANNER:
EXPLAIN SELECT username, contents FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';

-- VIEW BENCHMARKING INFO ON QUERY PLANS
-- THE VERY TOP QUERY PLAN STEP AS WELL AS OTHER QUERY PLANS WITH "->" (ARROW) SYMBOL IN THE BEGINING ARE QUERY NODES THAT PERFORM SOME DATA SEARCH OR DATA PROCESSING!
EXPLAIN ANALYZE SELECT username, contents FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';

OUTPUT:

"QUERY PLAN"
"Hash Join  (cost=8.31..1795.11 rows=11 width=81) (actual time=1.423..20.378 rows=7 loops=1)"
"  Hash Cond: (comments.user_id = users.id)"
"  ->  Seq Scan on comments  (cost=0.00..1628.10 rows=60410 width=72) (actual time=0.258..15.095 rows=60410 loops=1)"
"  ->  Hash  (cost=8.30..8.30 rows=1 width=17) (actual time=1.044..1.045 rows=1 loops=1)"
"        Buckets: 1024  Batches: 1  Memory Usage: 9kB"
"        ->  Index Scan using users_username_idx1 on users  (cost=0.28..8.30 rows=1 width=17) (actual time=1.037..1.038 rows=1 loops=1)"
"              Index Cond: ((username)::text = 'Alyson14'::text)"
"Planning Time: 5.650 ms"
"Execution Time: 20.997 ms"


THE ROWS WITH "->" ARROWS AT THE START IN THE QUERY PLAN OUTPUT ACT AS "QUERY NODES" WHERE SOME DATA SOTRED IN THE DATABASE IS ACCESSED
OR SOME PROCESSING IS DONE ON THE DATA IN THE DATABASE. THE VERY TOP ROW OF THE OUTPUT QUERY PLAN IS ALSO A "QUERY NODE". THE WAY TO
INTERPRET THIS OUTPUT QUERY PLAN FROM THE "EXPLAIN ANALYZE" STATEMENT, IS TO GO FROM THE INNERMOST ROW WITH THE "->" SYMBOL AND REACH THE
TOP ROW THROUGH THE "QUERY NODES". THIS CLOSELY RESEMBLES THE FLOW OF INFORMATION FROM THE DATABASE.
IN THIS PARTICULAR SCENARIO, INDEX SCANNING IS DONE ON THE username COLUMN INDEX OF THE users TABLE, WHICH IS THEN HASHED AND THEN JOINED WITH ALL THE ROWS OF TABLE FROM A 
SEQUENTIAL SCAN ON THE comments TABLE AND PRESENTED AS THE QUERY RESULT.
IN THE FINAL HASH JOIN STEP, THE cost IS THE AMOUNT OF PROCESSING POWER REQUIRED FOR THIS STEP, rows IS A GUESS OF HOW MANY ROWS OF DATA, THIS STEP WILL PRODUCT AND width
IS A GUESS OF THE AVERAGE NUMBER OF BYTES OF EACH ROW OF THE RESULT. THE actual_time INFO SHOWS UP ONLY IN "EXPLAIN ANALYZE" STATEMENT WHERE THE SQL STATEMENT IS EXECUTED
AND EXECUTION INFO IS DISPLAYED IN "QUERY PLAN". THIS IS IN CONTRAST TO JUST THE "EXPLAIN" STATEMENT ALONE WHERE THE UNDERLYING SQL STATEMENT WILL NOT BE EXECUTED AND ONLY
THE PREDICTED VALUES OF cost, rows, width ARE STORED IN THE "QUERY PLAN".

-- QUERY PLAN FROM EXPLAIN STATEMENT
EXPLAIN SELECT username, contents FROM users
JOIN comments ON comments.user_id = users.id
WHERE username = 'Alyson14';

OUTPUT:

"QUERY PLAN"
"Hash Join  (cost=8.31..1795.11 rows=11 width=81)"
"  Hash Cond: (comments.user_id = users.id)"
"  ->  Seq Scan on comments  (cost=0.00..1628.10 rows=60410 width=72)"
"  ->  Hash  (cost=8.30..8.30 rows=1 width=17)"
"        ->  Index Scan using users_username_idx1 on users  (cost=0.28..8.30 rows=1 width=17)"
"              Index Cond: ((username)::text = 'Alyson14'::text)"


-- PGADMIN ALSO HAS IN-BUILT UI BUTTON TO RUN "EXPLAN ANALYZE" TYPE BENCHMARKING WITH ADDITIONAL INFO. THIS CAN BE DONE IN A DROPDOWN NEXT TO THE QUERY EXECUTION BUTTON IN THE
-- SAME TOOLBAR.
-- DETAILED STATISTICS OF ALL THE COLUMNS OF ALL TABLES IN DATABASE FROM INTERNALLY MAINTAINED pg_stats TABLE IN POSTGRES
SELECT * FROM pg_stats;


-- POSTGRES PREDICTS THE cost, row and width VALUES FROM THE STORED CALCULATIONS IN THE INTERNALLY MAINTAINED pg_stats TABLE IN POSTGRES AND DISPLAYS IN THE "QUERY PLAN"
-- TO VIEW THE CALCULATED METRICS OF EVERY COLUMN OF THE users TABLE FROM THE INTERNAL pg_stats TABLE
SELECT * FROM pg_stats
WHERE tablename = 'users';


AS AN EXAMPLE OF THE IMPLICIT COST CALCULATION DONE BY POSTGRES, CONSIDER THE comments TABLE. IT HAS 60410 ROWS AND 1024 PAGES
-- GET TOTAL ROWS IN comments TABLE
SELECT COUNT(*) FROM comments

-- GET TOTAL NUMBER OF PAGES IN THE comments TABLE
SELECT relpages FROM pg_class WHERE relname = 'comments'; 

ASSUMING THAT IT COSTS 1 FOR EACH PAGE TO LOAD AND 0.01 FOR EACH ROW OF THE TABLE TO BE PROCESSED, WE HAVE: (1024 * 1) + (0.01 * 60410) = 1024 + 604.1 = 1628.1
1628.1 IS THE COST (BALLPARK VALUE FOR QUALITATIVE/RELATIVE COMPARISON) SHOWN IN THE "QUERY PLAN" AS WELL FOR SEQUENTIAL SCANNING OF THE COMMENTS TABLE.
IN SEQUENTIAL SCANNING, EVERY PAGE FROM THE START TO END OF THE HEAP FILE WILL BE LOADED. HOWEVER, IN INDEX-BASED SCANNING, WE WOULD LOAD AROUND 2 PAGES (ROOT PAGE AND THEN 
CORRESPONDING CHILD PAGE). COMPARED TO SEQUENTIAL SCANNING, THIS APPROACH LOADS RANDOM PAGES. THE COST OF LOADING THE SUCCESSIVE PAGE IN A HEAP FILE IS LOWER (DEFAULT: 1.0) THAN LOADING
A RANDOM PAGE FROM SOMEWHERE IN THE HEAP FILE (DEFAULT: 4.0) INTO MEMORY.

THE GENERAL FORMULA USED BY THE "PLANNER" STEP TO EVALUATE THE COST OF DIFFERENT "QUERY PLANS" IS AS FOLLOWS:

COST = (# PAGES READ SEQUENTIALLY) * SEQ_PAGE_COST (1.0)
	+ (# PAGES READ AT RANDOM) * RANDOM_PAGE_COST (4.0)
	+ (# ROWS SCANNED) * CPU_TUPLE_COST (0.1)
	+ (# INDEX ENTRIES SCANNED) * CPU_INDEX_TUPLE_COST (0.005)
	+ (# TIMES FUNCTION/OPERATOR EVALUATED) * CPU_OPERATOR_COST (0.0025)

THE VALUES FOR THE COST CONSTANTS ARE GIVEN IN: https://www.postgresql.org/docs/current/runtime-config-query.html

THE COSTS SHOWN IN THE "QUERY PLAN" USUALLY HAVE TWO NUMBERS SEPERATED BY A "..". THE FIRST NUMBER IS THE PREDICTED COST FOR THE STEP TO PRODUCE THE FIRST ROW.
THE SECOND NUMBER IS THE PREDICTED COST FOR THE STEP TO PRODUCE ALL ROWS. THE COST TO EMIT FIRST ROW FROM SEQUENTIAL SCAN IS THEORETICALLY 0.00 (COST IS A QUALITATIVE SCORE).
THE COST FOR HASHING STEP TO EMIT FIRST ROW AND ALL ROWS IS THE SAME BECAUSE THE HASHING STEP HASHES ALL ROWS AND EMITS THEM TOGETHER.
THE COST OF A PARENT NODE IS CALCULATED AS THE SUM OF THE COST OF ITS IMMEDIATE CHILDREN (COST FOR HASH JOIN IS COST OF HASH + COST OF SEQUENTIAL SCAN IN OUR "QUERY PLAN").


-- ANOTHER EXAMPLE OF THE QUERY EVALUATION PROCESS IN POSTGRES:
EXPLAIN SELECT * FROM likes 
WHERE created_at < '2013-01-01';

OUTPUT:
"QUERY PLAN"
"Seq Scan on likes  (cost=0.00..14248.11 rows=61476 width=24)"
"  Filter: (created_at < '2013-01-01 00:00:00+05:30'::timestamp with time zone)"

-- CREATE AN INDEX FOR likes.created_at
CREATE INDEX likes_created_at_idx ON likes (created_at);

-- RERUN ANALYSIS OF QUERY
EXPLAIN SELECT * FROM likes 
WHERE created_at < '2013-01-01';

OUTPUT:
"QUERY PLAN"
"Bitmap Heap Scan on likes  (cost=1152.86..6769.31 rows=61476 width=24)"
"  Recheck Cond: (created_at < '2013-01-01 00:00:00+05:30'::timestamp with time zone)"
"  ->  Bitmap Index Scan on likes_created_at_idx  (cost=0.00..1137.49 rows=61476 width=0)"
"        Index Cond: (created_at < '2013-01-01 00:00:00+05:30'::timestamp with time zone)"

-- FLIPPING THE WHERE CONDITION, WE SEE THAT A SEQUENTIAL SCAN IS USED INSTEAD OF AN INDEX
EXPLAIN SELECT * FROM likes 
WHERE created_at > '2013-01-01';

OUTPUT:
"QUERY PLAN"
"Seq Scan on likes  (cost=0.00..14248.11 rows=690532 width=24)"
"  Filter: (created_at > '2013-01-01 00:00:00+05:30'::timestamp with time zone)"

THIS SHOWS THAT INDEXES ARE BETTER FOR PICKING UP SMALL AMOUNTS OF DATA FROM THE DATABASE
BUT DUE TO ITS ASSOCIATED RANDOM PAGE FILE ACCESS COST, THE SEQUENTIAL SCANNING PROCESS BECOMES
MORE EFFICIENT WHEN RETRIEVING MORE THAN HALF OF THE RECORDS OF THE DATABASE.




COMMON TABLE EXPRESSIONS:



-- FIND NAME OF USERS WHO WERE TAGGED IN EITHER A CAPTION OR A PHOTO BEFORE JAN 7TH 2010 AND THE DATE THEY WERE TAGGED
-- UNION ALL KEEPS DUPLICATE ROWS AFTER UNION
-- IT IS IMPORTANT TO TELL SQL WHICH TABLE'S FIELD YOU WANT TO TAKE WHEN BOTH THE TABLES BEING JOINED HAVE THE SAME COLUMN NAME (tags.created_at)
SELECT username, tags.created_at
FROM users JOIN (SELECT user_id, created_at FROM photo_tags WHERE created_at < '2010-01-07'
UNION ALL
SELECT user_id, created_at FROM caption_tags WHERE created_at < '2010-01-07') AS tags
ON users.id = tags.user_id


-- THE SAME QUERY ABOVE CAN BE RE-WRITTEN USING COMMON TABLE EXPRESSIONS AND THE "WITH" STATEMENT
-- FOR BETTER READABILITY. THIS DOES NOT MODIFY THE "QUERY PLAN" IN ANY WAY
-- UNION ALL KEEPS DUPLICATE ROWS AFTER UNION
WITH tags AS (SELECT user_id, created_at FROM photo_tags WHERE created_at < '2010-01-07'
UNION ALL
SELECT user_id, created_at FROM caption_tags WHERE created_at < '2010-01-07')
SELECT username, tags.created_at
FROM users JOIN tags
ON users.id = tags.user_id



THE ABOVE QUERY IS A SIMPLE FORM OF COMMON TABLE EXPRESSIONS. THERE IS ALSO A RECURSIVE FORM OF COMMON TABLE EXPRESSIONS THAT IS USED
TO WRITE VERY POWERFUL QUERIES THAT WOULD OTHERWISE BE IMPOSSIBLE TO EXPRESS WITH NORMAL SQL!
RECURSIVE CTEs ARE USED EXTENSIVELY WITH TREE OR GRAPH TYPE DATA STRUCTURES.
EVERY RECURSIVE CTE WILL HAVE A "UNION" KEYWORD INSIDE OF IT!



-- THE RECURSIVE COMMON TABLE EXPRESSION QUERY CREATE TWO TABLES:
-- A RESULTS TABLE AND A WORKING TABLE. THE COLUMNS SPECIFIED INSIDE
-- countdown() IN THIS CASE WILL GO AS THE COLUMN IN BOTH THESE TWO TABLES.
-- THEN BOTH OF THESE TABLES ARE POPULATED WITH THE QUERY RESULTS OF THE 
-- NON-RECURSIVE QUERY STATEMENT.
-- THEN THE RECUSIVE STATEMENT USES THE WORKING TABLE TO SEND IN VALUES
-- TO APPEND TO THE RESULTS TABLE. WHEN THE RECURSIVE STATEMENT NO 
-- ROWS TO RETURN FROM THE WORKING TABLE, THE RECURSION TERMINATES
-- AND THE RESULTS TABLE IS RETURNED. 
-- THE ROWS OF THE WORKING TABLE ARE ALSO REPLACED WITH THE RESULT OF THE
-- RECURSIVE QUERY AFTER APPENDING THE VERY SAME RESULT TO THE RESULTS TABLE.
-- FINALLY, THE RESULTS TABLE IS RENAMED TO THE NAME OF THE GIVEN TABLE countdown
WITH RECURSIVE countdown(val) AS (
	SELECT 10 AS val -- INITIAL NON-RECURSIVE QUERY
	UNION
	SELECT val - 1 FROM countdown WHERE val > 1 -- RECURSIVE QUERY
)
SELECT * FROM countdown;



-- SUGGESTIONS TO FOLLOW FROM THE USERS FOLLOWED BY, THE USERS FOLLOWED
-- BY A USER
WITH RECURSIVE suggestions(leader_id, follower_id, depth) AS (
	SELECT leader_id, follower_id, 1 AS depth 
	FROM followers -- depth WILL BE USED AS TERMINATION FOR RECURSION
	WHERE follower_id = 1 -- THIS follower_id IS FOR THE USER FOR WHOM THIS QUERY IS RUN
	UNION
	SELECT followers.leader_id, followers.follower_id, depth + 1 -- TABLE NAME USER IN FIELDS TO PREVENT AMBIGUITY AFTER JOIN
	FROM followers
	JOIN suggestions ON suggestions.leader_id = followers.follower_id
	WHERE depth < 3
)
SELECT DISTINCT users.id, users.username 
FROM users JOIN suggestions
ON users.id = suggestions.leader_id
WHERE depth > 1
LIMIT 30;



-- FIND THE MOST POPULAR USERS WHO WERE TAGGED THE MOST
-- USING VIEWS
SELECT username, COUNT(*) 
FROM users
JOIN (
	SELECT user_id FROM photo_tags
	UNION ALL -- UNION ALL USED TO KEEP DUPLICATES
	SELECT user_id FROM caption_tags
) AS tags
ON tags.user_id = users.id
GROUP BY username
ORDER BY COUNT(*) DESC;



HAVING TO TAKE THE UNION OF THE TWO TAG TABLES photo_tags, caption_tags SEEMS TO BE DUE TO A DESIGN MISTAKE
IN OUR DATA DESIGN SINCE WE HAVE NOT SEEN ANY SITUATION TILL NOW WHERE THIS SEPERATION OF THE TAGS INTO
TWO TABLES WAS ADVANTAGEOUS. ONE POSSIBLE SOLUTION TO THIS IS THE MERGE THE TWO TABLES TO CREATE A NEW
TABLE AND THEN DELETE THE ORIGINAL ONES.

CREATE TABLE tags(
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
	post_id INTEGER NOT NULL REFERENCES posts(id) ON DELETE CASCADE,
	x INTEGER,
	y INTEGER
);

INSERT INTO tags(id, created_at, updated_at, user_id, post_id, x, y)
SELECT created_at, updated_at, user_id, post_id, x, y
FROM photo_tags;

INSERT INTO tags(created_at, updated_at, user_id, post_id)
SELECT created_at, updated_at, user_id, post_id
FROM caption_tags;

DROP TABLE photo_tags, caption_tags;

HOWEVER, THERE ARE TWO BIG DOWNSIDES TO THIS APPROACH: THE id COLUMN HAS TO BE UNIQUE FOR THE MERGED TABLE,
BUT WE WILL END UP WITH DUPLICATES AFTER INSERTING photo_tags AND caption_tags. SECONDLY, ANY EXISTING QUERIES
FOR THE TWO ORIGINAL TABLES WILL BREAK AFTER THE ORIGINAL TABLES ARE DROPPED!

THE SECOND SOLUTION IS TO CREATE A VIEW - A "VIEW" IS LIKE A FAKE TABLE THAT REFERENCES ROWS, CALCULATED VALUES OR DATA FROM OTHER
TABLES WITHIN THE SAME DATABASE. VIEWS ARE SIMILAR TO COMMON TABLE EXPRESSIONS EXCEPT THEY CAN BE REFERRED TO IN
DIFFERENT QUERIES - THEY ARE PERSISTENT! A "VIEW" IS TECHNICALLY A QUERY THAT GETS EXECUTED EVERYTIME YOU REFER TO IT!
WE CAN USE A VIEW IN ANY PLACE WHERE WE WOULD NORMALLY USE A TABLE. VIEWS DO NOT ACTUALLY CREATE A NEW TABLE OR MOVE DATA AROUND!

-- A "VIEW" IS TECHNICALLY A QUERY THAT GETS EXECUTED EVERYTIME YOU REFER TO IT!
-- CREATING A VIEW
CREATE VIEW tags AS (
	SELECT id, created_at, user_id, post_id, 'photo_tag' AS type FROM photo_tags
	UNION ALL -- UNION ALL USED TO ALLOW DUPLICATES IN id
	SELECT id, created_at, user_id, post_id, 'caption_tag' AS type FROM caption_tags
);

SELECT * FROM tags;

-- USING VIEWS FOR THE SOLUTION FIND THE MOST TAGGED USERS
SELECT username, COUNT(*) 
FROM users JOIN tags
ON users.id = tags.user_id
GROUP BY username -- REMEMBER THAT YOU CAN ONLY USE "GROUP BY" ON COLUMNS THAT YOU SELECT
ORDER BY COUNT(*) DESC;

-- ANOTHER TYPICAL EXAMPLE FOR THE USE OF VIEWS IS TO KEEP THE 10 MOST RECENT POSTS AS A VIEW TO
-- QUERY
CREATE VIEW recent_posts AS (
	SELECT * FROM posts
	ORDER BY created_at DESC
	LIMIT 10
);

SELECT * FROM recent_posts;


-- UPDATING recent_posts VIEW TO SHOW THE TOP 15 MOST RECENT POSTS
CREATE OR REPLACE VIEW recent_posts AS (
	SELECT * FROM posts
	ORDER BY created_at DESC
	LIMIT 15
);

SELECT * FROM recent_posts;


-- DELETING A VIEW
DROP VIEW recent_posts;


"MATERIALIZED VIEWS" ARE QUERIES THAT GET EXECUTED ONLY AT SPECIFIC TIMES INSTEAD OF EVERYTIME IT IS REFERRED TO. THE RESULTS OF
SUCH A VIEW IS STORED AND CAN BE REFERECED WITHOUT RE-RUNNING THE VIEW AS A QUERY EVERYTIME!
"MATERIALIZED VIEWS" CAN BE USED IN PLACE OF NORMAL VIEWS THAT TAKE TOO LONG TO EXECUTE!

-- AGGREGATE THE LIKES OF POSTS AND COMMENTS BY WEEK
-- POSTS ARE SEPERATE FROM COMMENTS AND LIKES OF BOTH ARE DIFFERENT,
-- RECALL THAT THE likes TABLE WILL HAVE EITHER post_id OR comment_id AS NULL
-- FOR EACH ROW IN IT!
-- WE CAN USE date_trunc() TO ROUND OFF DATE TIME VALUES TO THE NEAREST WEEK
-- REMEMBER THAT THE COALESCE() FUNCTION RETURNS THE FIRST NON-NULL VALUE
-- WE CAN HENCE USE COALESCE() TO GET THE created_at VALUE FOR A POST OR COMMENT
-- IRRESPECTIVE OF WHETHER THE LIKE WAS FOR A POST OR COMMENT
SELECT date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week, 
COUNT(posts.id) AS num_likes_for_posts, -- COUNT() DOES NOT COUNT NULL VALUES
COUNT(comments.id) as num_likes_for_comments -- COUNT() DOES NOT COUNT NULL VALUES
FROM likes
LEFT JOIN posts
ON posts.id = likes.post_id
LEFT JOIN comments
ON comments.id = likes.comment_id
GROUP BY week
ORDER BY week;

-- CREATE "MATERIALIZED VIEW" FOR THE EXPENSIVE QUERY ABOVE - IMPROVE EXECUTION SPEED
CREATE MATERIALIZED VIEW weekly_likes AS (
	SELECT date_trunc('week', COALESCE(posts.created_at, comments.created_at)) AS week, 
	COUNT(posts.id) AS num_likes_for_posts, -- COUNT() DOES NOT COUNT NULL VALUES
	COUNT(comments.id) as num_likes_for_comments -- COUNT() DOES NOT COUNT NULL VALUES
	FROM likes
	LEFT JOIN posts
	ON posts.id = likes.post_id
	LEFT JOIN comments
	ON comments.id = likes.comment_id
	GROUP BY week
	ORDER BY week
) WITH DATA;

-- NOW WE CAN USE THE "MATERIALIZED VIEW" TO THE EXACT DATA THAT WE WANT QUICKLY
SELECT * FROM weekly_likes;

ONE DOWNSIDE TO USING "MATERIALIZED VIEWS" IS THAT IF THE UNDERLYING TABLES likes, comments, posts ETC ARE
MODIFIED OR UPDATED, THIS WILL NOT BE REFLECTED IN THE CACHED RESULTS OF THE "MATERIALIZED VIEW". A SEPERATE
STATEMENT HAS TO BE RUN TO TELL THE "MATERIALIZED VIEW" TO GET UPDATED DATA BY RE-RUNNING ITS QUERY!

-- DELETE SOME ROWS FROM posts TABLE
DELETE FROM posts 
WHERE created_at < '2010-02-01';

-- THE DELETION IS NOT REFLECTED IN OUR MATERIALIZED VIEW
SELECT * FROM weekly_likes;

-- UPDATE THE MATERIALIZED VIEW weekly_likes WITH LATEST DATA FROM THE UNDERLYING TABLES
REFRESH MATERIALIZED VIEW weekly_likes;

-- THE DELETION IS NOT REFLECTED IN OUR MATERIALIZED VIEW
SELECT * FROM weekly_likes;


EACH QUERY TOOL WINDOW IS A CONNECTION TO THE POSTGRES DATABASE. THERE CAN BE MULTIPLE CONNECTIONS
OPEN FOR A DATABASE AND EACH CONNECTION CAN HAVE ACCESS TO ALL THE DATA OF THE DATABASE. 
HAVING MULTIPLE QUERIES BEING RUN SIMULTANEOUSLY THROUGH DIFFERENT CONNECTIONS ON THE SAME DATABASE WILL
CAUSE ISSUES IN DATA INTEGRITY. "TRANSACTIONS" ARE USED IN SUCH CASES TO MAINTAIN THE ACID PROPERTIES OF
THE DATABASE. A "TRANSACTION" IS A BUNDLE OF SQL STATEMENTS THAT SUCCESSFULLY EXECUTE ONLY WHEN ALL THE STATEMENT
IN THE "TRANSACTION" ARE EXECUTED SUCCESSFULLY. EVEN IF ONE OF THE STATEMENTS OF THE "TRANSACTION" FAILS, THE ENTIRE
"TRANSACTION" IS "ROLLED-BACK". EVERY "TRANSACTION" ACTS ON A PARTICULAR STATE OF THE DATABASE AND THERE CAN ONLY
BE ONE TRANSACTION EXECUTED ON THE DATABASE AT A TIME!

TRANSACTIONS FOLLOW FOUR STANDARD PROPERTIES, OUTLINED BY THE ACRONYM "ACID":
 * ATOMICITY - ENSURES THAT ALL OPERATIONS WITHIN THE WORK UNIT ARE COMPLETED SUCCESSFULLY. OTHERWISE, THE TRANSACTION IS ABORTED AT THE POINT OF FAILURE AND ALL THE PREVIOUS
OPERATIONS ARE ROLLED BACK TO THEIR FORMER STATE
 * consistency − ENSURES THAT THE DATABASE PROPERLY CHANGES STATE UPON A SUCCESSFULLY COMMITTED TRANSACTION
 * ISOLATION - ENABLES TRANSACTIONS TO OPERATE INDEPENDENTLY OF AND TRANSPARENT TO EACH OTHER
 * DURABILITY - ENSURES THAT THE RESULT OR EFFECT OF A COMMITTED TRANSACTION PERSISTS IN CASE OF A SYSTEM FAILURE


-- CREATE NEW TABLE FOR TRANSACTIONS
CREATE TABLE accounts (
	id SERIAL PRIMARY KEY,
	name VARCHAR(20) NOT NULL,
	balance INTEGER NOT NULL
);

-- POPULATE THE NEW TABLE
INSERT INTO accounts(name, balance)
VALUES ('Gia', 100), ('Allyson', 100);

IF YOU OPEN TWO "QUERY TOOL" WINDOWS FOR THE SAME DATABASE AND EXECUTE THE FOLLOWING IN ONE OF THE WINDOWS:
BEGIN; -- OPEN A TRANSACTION IN THE CURRENT CONNECTION

IT WILL INITIATE AN ISOLATED WORKSPACE WITH THE CURRENT STATE OF THE DATABASE FOR THE START OF A "TRANSACTION" IN THE PARTICULAR WORKSPACE OF CONNECTION/QUERY TOOL WINDOW
WHERE THE ABOVE STATEMENT WAS EXECUTED.

-- RUN THIS STATEMENT IN THE SAME CONNECTION WHERE THE "TRANSACTION" WAS INITIATED
UPDATE accounts
SET balance = balance - 50
WHERE name = 'Allyson';

-- CHECK THE UPDATED DATA
SELECT * FROM accounts;

-- NOW CHECK FROM THE SECOND CONNECTION WHERE A "TRANSACTION" WAS NOT INITIATED IF THE DATA UPDATE IS REFLECTED.
-- THE DATABASE UPDATION IN THE FIRST CONNECTION WITH A "TRANSACTION" INITIATED WILL NOT BE SEEN IN THE SECOND CONNECTION
-- THIS IS BECAUSE EACH CONNECTION IS ISOLATED FROM ALL THE OTHER CONNECTIONS TO A DATABASE
SELECT * FROM accounts;

-- SECOND DATA UPDATE STATEMENT IN "TRANSACTION" CONNECTION
UPDATE accounts
SET balance = balance + 50
WHERE name = 'Gia';



TO MAKE THE STATE CHANGES IN THE DATABASE ACESSIBLE TO ALL OTHER CONNECTIONS, WE HAVE TO MERGE THE STATE CHANGES FROM THE "TRANSACTION" CONNECTION
BY COMMITING THE STATE CHANGES TO THE DATABASE / MAIN DATA POOL USING THE "COMMIT" KEYWORD. THIS MAKES THE NEW STATE OF THE DATABASE AVAILABLE TO ALL FURTHER TRANSACTIONS
ALTERNATIVELY, IF YOU DO NOT WANT TO MERGE THE CHANGES FROM THE OPEN TRANSACTION TO THE MAIN DATA POOL / DATABASE, YOU CAN USE THE "ROLLBACK" KEYWORD TO CANCEL 
THE "TRANSACTION" AND DELETE ITS WORKSPACE.RUNNING A WRONG SQL STATEMENT/COMMAND IN A "TRANSACTION" WILL PUT THE TRANSACTION IN AN "ABORTED" STATE WHICH REQUIRES MANUALLY RUNNING 
THE "ROLLBACK" OPERATION TO BRING THE DATABASE BACK TO ITS OPERATIONAL STATE. NO SQL STATEMENTS EXCEPT "ROLLBACK" CAN BE EXECUTED AGAINST AN "ABORTED" STATE DATA POOL. 
SIMILARLY, IF A CONNECTION CRASHES OR SHUTS DOWN DUE TO ANY ISSUE WHILE EXECUTING A TRANSACTION, POSTGRES WILL AUTOMATICALLY DETECT IT AND PERFORM THE "ROLLBACK" OPERATION 
TO BRING THE DATABASE BACK TO THE MAIN STATE AVAILABLE TO ALL OTHER CONNECTIONS.
IN A "TRANSACTION", EITHER ALL OF THE SQL STATEMENTS ARE EXECUTED AND THE RESULT, COMMITTED, OR NONE OF THE SQL STATEMENTS ARE EXECUTED!

-- COMMIT THE MODIFICATIONS DONE TO THE DATABASE THROUGH THE CONNECTION WHERE THE TRANSACTION WAS INITIALIZED
COMMIT;

-- NOW THE MODIFIED DATABASE WILL BE AVAILABLE FOR BOTH THE CONNECTIONS
SELECT * FROM accounts;




TWO IMPORTANT THINGS TO KEEP IN MIND WHEN PERFORMING DATABASE MIGRATIONS (CLONING DATABASES IN DIFFERENT ENVIRONMENTS OR ALTERING DATA - DATA MIGRATION / UPDATING DATABASE STRUCTURE - SCHEMA MIGRATION):
 * CHANGES TO THE DATABASE STRUCTURE AND CHANGES TO CLIENTS/APIs USING THE DATABASE NEED TO BE MADE PRECISELY AT THE SAME TIME
   WHEN WORKING WITH OTHER ENGINEERS, WE NEED A REALLY EASY WAY TO TIE STRUCTURE OF THE DATABASE WITH THE CODE BASE - A SCHEMA MIGRATION FILE CONTAINING CODE
 * THAT DESCRIBES THE PRECISE CHANGE TO MAKE TO THE DATABASE FOR EXAMPLE. SUCH A SCHEMA MIGRATION FILE CAN BE WRITTEN IN ANY PROGRAMMING LANGUAGE (ALL POPULAR PROGRAMMING 
   LANGUAGES HAVE LIBRARIES FOR CREATING/RUNNING DATABASE SCHEMA MIGRATION). HOWEVER, IT IS HIGHLY RECOMMENDED TO USE PLAIN SQL FOR MIGRATION FILES. 
   THE MIGRATION FILE WILL CONTAIN STATEMENTS THAT UPGRADE THE DATABASE STRUCTURE TO THE DESIRED STATE (THE "UP COMMAND") AS WELL AS CORRESPONDING STATEMENTS TO 
   "ROLLBACK" OR DOWNGRADE THESE DATABASE MODIFICATIONS (UNDO THE "UP COMMAND")

IT IS GOOD PRACTICE TO PERFORM DATA MIGRATION AND SCHEMA MIGRATION IN SEPERATE MIGRATION STEPS / MIGRATION FILES / MIGRATION TRANSACTION STEPS! IN THE REAL WORLD WHERE
DATABASES HAVE MILLIONS OF RECORDS, DATA MIGRATIONS WILL TAKE A LONG TIME TO COMPLETE AND WHEN THE TRANSACTION IS FINALLY COMMITTED TO THE MAIN DATABASE, THERE MIGHT HAVE ALREADY
BEEN ADDITIONAL DATA ADDED TO THE DATABASE FROM API CALLS. THESE ADDITIONAL ROWS WILL CAUSE ISSUES WHEN THE TRANSACTION IS TRYING TO GET MERGED: IT CAN SOMETIMES LEAD TO DATA LOSS
IN THE NEW COLUMNS CREATED IN THE TRANSACTION (NULL VALUE INSERTION) OR EVEN BREAK THE DATABASE ITSELF!

AS AN EXAMPLE FOR MIGRATION, IF YOU WANT TO ADD A loc COLUMN TO THE posts TABLE TO REPLACE THE lat, lng COLUMNS, THEN A SMOOTH WAY TO GET THIS MIGRATION GOING IS AS FOLLOWS:
 * AS A FIRST MIGRATION STEP, ADD THE NEW COLUMN loc TO THE posts TABLE
 * THEN, DEPLOY A NEW VERSION OF THE API THAT WILL WRITE TO BOTH THE loc AND lat, lng COLUMNS OF THE posts TABLE
 * THE NEXT MIGRATION STEP IS TO BACKFILL THE loc COLUMN (WHICH WILL HAVE NULL VALUES FOR THE ROWS CREATED BY THE OLDER API) VALUES WHICH ARE NOT POPULATED USING THE CORRESPONDING
   lat, lng VALUES FOR THE ROWS WITH NULL VALUES IN loc.
 * UPDATE THE API TO NOW WRITE ONLY THE THE loc COLUMN AND IGNORE THE lat, lng COLUMN OF THE TABLE (FILL WITH NULL)
 * FINALLY, DROP THE lat, lng COLUMNS FROM THE posts TABLE


-- CREATE A NEW posts TABLE
CREATE TABLE posts (
	id SERIAL PRIMARY KEY,
	url VARCHAR(300),
	lat NUMERIC,
	lng NUMERIC
)


-- SCHEMA MIGRATION1: ADDING A NEW COLUMN (OF TYPE POINT) TO AN EXISTING TABLE
ALTER TABLE posts
ADD COLUMN loc POINT;

-- UNDO FOR SCHEMA MIGRATION1: DROP AN EXISTING COLUMN FROM A TABLE
ALTER TABLE posts
DROP COLUMN loc;

-- INSERT VALUES INTO THE posts TABLE
INSERT INTO posts (lat, lng, loc) VALUES (1, 2, (1, 2));



SUPPOSE WE WANT TO COPY OVER THE lat AND lng VALUES INTO THE loc COLUMN ALL TOGETHER, WE HAVE A LOT OF OPTIONS TO GO ABOUT IT.

-- OPTION1
-- THIS QUERY HAS THE POTENTIAL TO CRASH THE NODE SERVER BECAUSE THE posts TABLE COULD HAVE MILLIONS OF RECORDS!
-- EVEN BATCHING USING LIMIT AND OFFSET IN THE QUERY THROUGH JAVASCRIPT LOOP, COULD FAIL AND LEAVE US IN A HALFWAY-BETWEEN STATE!
-- THE DATABASE CAN ALSO BE LOCKED DOWN AND UNAVAIALBE TO CONNECT THOUGH A PROGRAMMING ENVIRONMENT!
SELECT id, lat, lng
FROM posts
WHERE loc IS NULL;

-- RUN BUSINESS LOGIC/VALIDATION IN WEB SERVER/JAVASCRIPT BACKEND AND THEN EXECUTE THE
-- FOLLOWING QUERY
UPDATE posts 
SET loc = ...
WHERE id = ...

-- OPTION2:
-- USE SQL DIRECTLY FOR MIGRATION AND VALIDATION
-- BUT IS BECOMES HARDER TO IMPLEMENT BUSINESS LOGIC FOR THE VALIDATION
UPDATE posts
SET loc = ...
WHERE loc IS NULL;


AN ISSUE THAT AFFECTS BOTH OPTIONS IS TRANSACTION LOCK, WHERE ALL THE RECORDS IN THE TABLE
ARE LOCKED UNTIL THE UPDATION OF loc COLUMN HAPPENS. THIS WILL TAKE A LONG TIME FOR TABLES WITH
VERY LARGE NUMBER OF RECORDS AND BLOCKS OUT USERS FROM RUNNING OTHER UPDATE STATEMENTS ON THE TABLE!
WE CAN SIMULATE THIS BY USING TWO SEPERATE QUERY TOOL WINDOWS AND RUNNING THE FOLLOWING SQL STATEMENTS
IN THEM:

-- STATEMENT FOR QUERY TOOL WINDOW 1
-- START A TRANSACTION, BUT NEVER END IT (COMMIT IS NOT DONE)
BEGIN;
UPDATE posts 
SET lat = 2 
WHERE id = 1;


-- STATEMENT FOR QUERY TOOL WINDOW 2
-- THIS QUERY WINDOW WILL WAIT FOR WINDOW 1 TO FINISH WHICH WILL NEVER HAPPED
-- BECAUSE THE TRANSACTION IN WINDOW 1 WILL NEVER TERMINATE AND THE ROW BEING
-- ACCESSED FOR UPDATION IN WINOW 2 IS LOCKED WITH TRANSACTION FROM WINDOW 1
UPDATE posts 
SET lat = 10 
WHERE id = 1;


USING A BATCHED UPDATE STATEMENT IN WINDOW 1 USING LIMIT AND OFFSET CAN BE USED TO RESOLVE THIS
TRANSACTION LOCKING, BUT EACH TRANSACTION WILL HAVE TO UPDATE ONLY A CHUNK OF THE TABLE THROUGH RAW
SQL BY RUNNING IT MANUALLY FOR EACH CHUNK.


NEXT, WE UPDATE THE API USING THE TABLE TO ONLY INSERT INTO THE loc COLUMN AND LEAVE NULL VALUES FOR lat AND lng
THEN, WE ROLLOUT THE MIGRATION FOR DROPPING THE lat AND lng COLUMNS FROM THE posts TABLE

-- DROP lat AND lng COLUMNS
ALTER TABLE posts
DROP COLUMN lat,
DROP COLUMN lng;


-- UNDO STATEMENT FOR DROPPING lat AND lng
ALTER TABLE posts
ADD COLUMN lat NUMERIC,
ADD COLUMN lng NUMERIC;



ANOTHER EXAMPLE
-- MIGRATION1:
CREATE TABLE users (
	id SERIAL PRIMARY KEY,
	created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
	bio VARCHAR(400),
	username VARCHAR(30) NOT NULL
);

-- UNDO FOR MIGRATION1:
DROP TABLE users;


UESEFUL TIPS FOR REST APIs THAT PERFORM DATABASE OPERATIONS IN THE BACKEND:

 * INSTEAD OF RUNNING THE SQL STATEMENTS REQUIRED DIRECTLY THROUGH THE ROUTE HANDLERS, CONSIDER CREATING A REPOSITORY CLASS WITH (STATIC) METHODS FOR EACH SQL STATEMENT THAT SERVES
   A PURPOSE (LIST ALL USERS, FIND A USER BY ID, DELETE EXISTING USER, INSERT NEW USER, UPDATE USER DETAILS ETC.) AND USE THE REPOSITORY METHODS INSIDE THE ROUTE HANDLER.
   THE REPOSITORY CLASS CAN ALSO HAVE ADDITIONAL METHODS THAT ARE NOT USED IN THE ROUTE HANDLER SUCH AS SQL INJECTION PREVENTION METHODS OR VALIDATION METHODS, AND METHODS TO 
   CONVERT TABLE HEADER NAMES TO CAMEL-CASE IN THE QUERY RESULT OBJECT.
 * WHEN CREATING REST APIs IN JAVASCRIPT THAT RUN SQL CODE IN THE BACKEND (POSTGRESQL APIs) TO MODIFY/QUERY A DATABASE, REMEMBER TO SANITIZE THE USER INPUT/INTERPOLATED QUERY VALUES   
   RECIEVED TO PREVENT SQL INJECTIONS. MAKE A PREPARED STATEMENT IN THE REPOSITORY CLASS METHOD THAT VALIDATES USER INPUT PARAMETERS AND PREPARES A VALID, SECURE SQL STATEMENT TO
   BE EXECUTED IN THE DATABASE. THE .query() METHOD OF JAVASCRIPT POSTGRES CLIENT FOR EXAMPLE, TAKES THE SQL COMMAND AND A LIST OF PARAMETERS WHICH IT VALIDATES FOR DATA TYPE
   BASED ON THE PLACEHOLDER FOR THE PARAMETER IN THE SQL QUERY STATEMENT.
   NOTE THAT YOU CANNOT USE COLUMN IDENTIFIERS OR TABLE NAMES ON THE PREPARED SQL ON THE FLY LIKE YOU PROVIDE THE PARAMETERS FOR THE QUERY! 
 * THESE POSTGRESQL APIs CREATED IN JAVASCRIPT BACKEND CAN BE TESTED USING POSTMAN / VSCode REST CLIENT
 * REMEMBER TO VALIDATE THE REQUEST BODY OF POST REQUESTS AS WELL IN THE ROUTE HANDLERS FOR THE REQUEST! THE ROUTE HANLDER WILL THEN DESTRUCTURE THE VALIDATED REQUEST, AND SEND
   TO THE APPROPRIATE REPOSITORY CLASS METHOD THAT WILL EXECUTE A SECURE, PREPARED SQL STATEMENT.
 * YOU CAN USE THE 'RETURNING' KEYWORD IN SQL INSERT/UPDATE/DELETE STATEMENTS TO GET BACK VALUES AFTER EXECUTING SUCH STATEMENTS AND PASS IT TO THE ROUTE HANDLER AFTER SOME 
   PROCESSING, SUCH AS CAMEL-CASE CONVERSION, FROM THE REPOSITORY METHOD. THIS INFORMATION WILL BE MOSTLY USED IN POST/PUT/DELETE REQUEST HANDLERS TO ENSURE THAT A ROW WITH 
   THE GIVEN ID/PRIMARY KEY WAS MODIFIED/DELETED BY CHECKING FOR UNDEFINED VALUES IN THE RETURNED RESULT OF REPOSITORY METHOD.
 * WHEN RUNNING FAST PARALLEL TESTS FOR THESE APIs USING MULTIPLE TEST FILES/SUITES, IT IS BETTER TO HAVE MULTIPLE INDEPENDENT COPIES OF THE DATABASE FOR EACH TEST FILE, TO PREVENT DATA
   LOCKING AND ACCESS COLLISION BETWEEN THE PARALLEL TESTS. HOWEVER, WHEN THERE ARE A LARGE NUMBER OF TEST FILES/SUITES, IT BECOMES TEDIOUS TO SETUP AN INDEPENDENT COPY OF THE DATABASE
   WITH APPLIED MIGRATIONS. A MUCH BETTER WAY TO WORK WITH MULTIPLE PARALLEL TEST FILES/SUITES IS TO LET EACH TEST FILE/SUITE TO HAVE ITS OWN SCHEMA. SCHEMAS ARE LIKE FOLDERS TO 
   ORGANIZE THINGS IN A DATABASE. RANDOMLY GENERATE AND ASSIGN A ROLE NAME TO CONNECT TO POSTGRES AS IN EACH TEST FILE/SUITE, CREATE A SCHEMA WITH THE SAME NAME AS THE ROLE,
   THEN RUN THE TESTS IN THE FILE/SUITE WITH THE CORRESPONDING USING CORRESPONDING SCHEMA. EACH TEST FILE/SUITE GETS ITS OWN INDEPENDENT SCHEMA WITH SEPERATE ACCESS ROLES
   EVEN THOUGH THE TABLES USED IN THE SQL COMMANDS REMAIN EXACTLY THE SAME.
   REMEMBER TO DISCONNECT FROM CURRENT ROLE, LOGIN AS ROOT USER, DELETE ROWS FROM THE TABLE IN CURRENT SCHEMA, DELETE THE CURRENT SCHEMA AND ROLE CREATED FOR THE SPECIFIC TEST FILE/SUITE 
   AND THEN DISCONNECT FROM THE DATABSE. PERFORM THIS CLEANUP PROCESS IN EACH TEST FILE/SUITE.



EVERY DATABASE IN POSTGRES GETS A DEFAULT SCHEMA (NOT TO BE CONFUSED WITH TABLE SCHEMA) CALLED 'public'. EACH SCHEMA CAN HAVE ITS OWN SEPERATE COPY OF A TABLE. BY DEFAULT, ALL QUERIES ARE
RUN ON TABLES IN THE 'public' SCHEMA.  WHEN WE WANT TO PERFORM A QUERY ON A TABLE ON A DIFFERENT SCHEMA, WE SPECIFY THE TALBE NAME IS THE SQL 
STATEMENT AS <schema>.<table_name>

-- USING test SCHEMA TABLES
SELECT * FROM test.users;

INSERT INTO test.users (username)
VALUES ('alex'),
       ('asdadwqedqw');

POSTGRES USES THE search_path INTERNAL SETTING TO STORE THE NAMES AND PRECEDENCE OF THE SCHEMAS THAT IT USES BY DEFAULT TO SEARCH FOR GIVEN TABLES IN EXECUTED STATEMENTS.
WE CAN MODIFY THIS INTERNAL SETTING TO CHANGE THE PRECEDENCE OR ADD ADDITIONAL SCHEMAS IN THE DEFAULT SEARCH SPACE.

-- QUERY TO CHECK THE DEFAULT SCHEMA
SHOW search_path;

-- ADD test SCHEMA INTO THE TOP OF THE DEFAULT SCHEMA LIST
-- THIS MAKES POSTGRES SEACH FOR A GIVEN TABLE NAME IN THE test SCHEMA FIRST AND THEN CHECK
-- IN THE public SCHEMA
SET search_path TO test, public;

-- RESTORE search_path SETTING
SET search_path TO "$user", public;


A ROLE IN POSTGRES IS AN ENTITY THAT CAN OWN DATABASE OBJECTS AND HAVE DATABASE PRIVILEGES. IT CAN BE BE CONSIDERED TO BE SIMILAR TO A USER OR A GROUP.
NOTE THAT ROLES ARE DEFINED AT THE DATABASE CLUSTER LEVEL, AND SO ARE VALID IN ALL DATABASES IN THE CLUSTER.


-- CREATE A NEW ROLE THAN CAN LOGIN TO THE DATABASE USING A PRE-DEFINED PASSWORD
CREATE ROLE test_role WITH LOGIN PASSWORD 'test';

-- CREATE A SCHEMA TO CURRENT ROLE
CREATE SCHEMA test_schema AUTHORIZATION test_role;

-- DELETE A SCHEMA
DROP SCHEMA test_schema CASCADE;

-- DELETE A ROLE
DROP ROLE test_role;